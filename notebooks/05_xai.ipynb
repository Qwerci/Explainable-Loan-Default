{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cec28c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\theow\\Documents\\Project\\Explainable-Loan-Default\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32b22299",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(r\"C:\\Users\\theow\\Documents\\Project\\Explainable-Loan-Default\\models\\best_xgb_model.pkl\")  \n",
    "X_train = pd.read_csv(r\"C:\\Users\\theow\\Documents\\Project\\Explainable-Loan-Default\\data\\processed/X_train.csv\")\n",
    "y_train = pd.read_csv(r\"C:\\Users\\theow\\Documents\\Project\\Explainable-Loan-Default\\data\\processed\\y_train.csv\").values.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70288385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure X_train is a DataFrame (not numpy)\n",
    "if not isinstance(X_train, pd.DataFrame):\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "\n",
    "# Select only categorical columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# One-hot encode categorical features if any\n",
    "if categorical_cols:\n",
    "    encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "    encoded = encoder.fit_transform(X_train[categorical_cols])\n",
    "    encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(categorical_cols), index=X_train.index)\n",
    "\n",
    "    # Drop original categorical and add encoded\n",
    "    X_train = X_train.drop(columns=categorical_cols)\n",
    "    X_train = pd.concat([X_train, encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cf6220f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot cast array data from dtype('O') to dtype('float64') according to the rule 'safe'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m explainer = \u001b[43mshap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mExplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m shap_values = explainer(X_train)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\theow\\Documents\\Project\\Explainable-Loan-Default\\env\\Lib\\site-packages\\shap\\explainers\\_explainer.py:249\u001b[39m, in \u001b[36mExplainer.__init__\u001b[39m\u001b[34m(self, model, masker, link, algorithm, output_names, feature_names, linearize_link, seed, **kwargs)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m algorithm == \u001b[33m\"\u001b[39m\u001b[33mtree\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    248\u001b[39m     \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m = explainers.TreeExplainer\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m     \u001b[43mexplainers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTreeExplainer\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmasker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlink\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlinearize_link\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlinearize_link\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m algorithm == \u001b[33m\"\u001b[39m\u001b[33madditive\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    259\u001b[39m     \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m = explainers.AdditiveExplainer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\theow\\Documents\\Project\\Explainable-Loan-Default\\env\\Lib\\site-packages\\shap\\explainers\\_tree.py:278\u001b[39m, in \u001b[36mTreeExplainer.__init__\u001b[39m\u001b[34m(self, model, data, model_output, feature_perturbation, feature_names, approximate, link, linearize_link)\u001b[39m\n\u001b[32m    276\u001b[39m \u001b[38;5;28mself\u001b[39m.feature_perturbation = feature_perturbation\n\u001b[32m    277\u001b[39m \u001b[38;5;28mself\u001b[39m.expected_value = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mTreeEnsemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_missing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28mself\u001b[39m.model_output = model_output\n\u001b[32m    280\u001b[39m \u001b[38;5;66;03m# self.model_output = self.model.model_output # this allows the TreeEnsemble to translate model outputs types by how it loads the model\u001b[39;00m\n\u001b[32m    281\u001b[39m \n\u001b[32m    282\u001b[39m \u001b[38;5;66;03m# check for unsupported combinations of feature_perturbation and model_outputs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\theow\\Documents\\Project\\Explainable-Loan-Default\\env\\Lib\\site-packages\\shap\\explainers\\_tree.py:1255\u001b[39m, in \u001b[36mTreeEnsemble.__init__\u001b[39m\u001b[34m(self, model, data, data_missing, model_output)\u001b[39m\n\u001b[32m   1253\u001b[39m \u001b[38;5;28mself\u001b[39m.input_dtype = np.float32\n\u001b[32m   1254\u001b[39m \u001b[38;5;28mself\u001b[39m.original_model = model.get_booster()\n\u001b[32m-> \u001b[39m\u001b[32m1255\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_xgboost_model_attributes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_missing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjective_name_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtree_output_name_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1260\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_output == \u001b[33m\"\u001b[39m\u001b[33mpredict_proba\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1263\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_stacked_models == \u001b[32m1\u001b[39m:\n\u001b[32m   1264\u001b[39m         \u001b[38;5;66;03m# with predict_proba we need to double the outputs to match\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\theow\\Documents\\Project\\Explainable-Loan-Default\\env\\Lib\\site-packages\\shap\\explainers\\_tree.py:1500\u001b[39m, in \u001b[36mTreeEnsemble._set_xgboost_model_attributes\u001b[39m\u001b[34m(self, data, data_missing, objective_name_map, tree_output_name_map)\u001b[39m\n\u001b[32m   1497\u001b[39m \u001b[38;5;28mself\u001b[39m.model_type = \u001b[33m\"\u001b[39m\u001b[33mxgboost\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1498\u001b[39m loader = XGBTreeModelLoader(\u001b[38;5;28mself\u001b[39m.original_model)\n\u001b[32m-> \u001b[39m\u001b[32m1500\u001b[39m \u001b[38;5;28mself\u001b[39m.trees = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_trees\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_missing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_missing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1501\u001b[39m \u001b[38;5;28mself\u001b[39m.base_offset = loader.base_score\n\u001b[32m   1502\u001b[39m \u001b[38;5;28mself\u001b[39m.objective = objective_name_map.get(loader.name_obj, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\theow\\Documents\\Project\\Explainable-Loan-Default\\env\\Lib\\site-packages\\shap\\explainers\\_tree.py:2239\u001b[39m, in \u001b[36mXGBTreeModelLoader.get_trees\u001b[39m\u001b[34m(self, data, data_missing)\u001b[39m\n\u001b[32m   2229\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_trees):\n\u001b[32m   2230\u001b[39m     info = {\n\u001b[32m   2231\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mchildren_left\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.node_cleft[i],\n\u001b[32m   2232\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mchildren_right\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.node_cright[i],\n\u001b[32m   (...)\u001b[39m\u001b[32m   2237\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnode_sample_weight\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.sum_hess[i],\n\u001b[32m   2238\u001b[39m     }\n\u001b[32m-> \u001b[39m\u001b[32m2239\u001b[39m     trees.append(\u001b[43mSingleTree\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_missing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_missing\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2240\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trees\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\theow\\Documents\\Project\\Explainable-Loan-Default\\env\\Lib\\site-packages\\shap\\explainers\\_tree.py:1971\u001b[39m, in \u001b[36mSingleTree.__init__\u001b[39m\u001b[34m(self, tree, normalize, scaling, data, data_missing)\u001b[39m\n\u001b[32m   1969\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m data_missing \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1970\u001b[39m     \u001b[38;5;28mself\u001b[39m.node_sample_weight.fill(\u001b[32m0.0\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1971\u001b[39m     \u001b[43m_cext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdense_tree_update_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1972\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchildren_left\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1973\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchildren_right\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1974\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchildren_default\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1975\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1976\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mthresholds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1977\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1978\u001b[39m \u001b[43m        \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1979\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnode_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1980\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1981\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_missing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1982\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1984\u001b[39m \u001b[38;5;66;03m# we compute the expectations to make sure they follow the SHAP logic\u001b[39;00m\n\u001b[32m   1985\u001b[39m \u001b[38;5;28mself\u001b[39m.max_depth = _cext.compute_expectations(\n\u001b[32m   1986\u001b[39m     \u001b[38;5;28mself\u001b[39m.children_left, \u001b[38;5;28mself\u001b[39m.children_right, \u001b[38;5;28mself\u001b[39m.node_sample_weight, \u001b[38;5;28mself\u001b[39m.values\n\u001b[32m   1987\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: Cannot cast array data from dtype('O') to dtype('float64') according to the rule 'safe'"
     ]
    }
   ],
   "source": [
    "explainer = shap.Explainer(model, X_train)\n",
    "shap_values = explainer(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bda7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values, max_display=15)\n",
    "plt.title(\"SHAP Summary Plot (Global Importance)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50975c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP bar plot\n",
    "shap.plots.bar(shap_values, max_display=15)\n",
    "plt.title(\"Feature Importance - SHAP Bar Plot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddecb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cohort Analysis Using SHAP Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963554e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cohort column for example (you can adjust based on relevant features)\n",
    "X_train['income_bracket'] = pd.cut(X_train['person_income'], bins=[0, 30000, 70000, 150000, np.inf], \n",
    "                                   labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "# Select cohort\n",
    "cohort = 'income_bracket'\n",
    "\n",
    "# Generate SHAP values for each cohort\n",
    "for bracket in X_train[cohort].unique():\n",
    "    cohort_data = X_train[X_train[cohort] == bracket]\n",
    "    cohort_shap_values = explainer(cohort_data)\n",
    "    \n",
    "    # Plot SHAP summary plot for each cohort\n",
    "    shap.plots.beeswarm(cohort_shap_values, max_display=15)\n",
    "    plt.title(f\"SHAP Summary Plot for {bracket} Income Bracket\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1486f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAP Dependence Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63560ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP dependence plot for 'person_income'\n",
    "shap.plots.scatter(shap_values[:, \"person_income\"], color=shap_values)\n",
    "plt.title(\"SHAP Dependence Plot for 'person_income' Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a2f603",
   "metadata": {},
   "outputs": [],
   "source": [
    "Local Explainability for Individual Prediction (Waterfall Plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da5b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a sample to explain (for example, the first sample in the training set)\n",
    "sample = X_train.iloc[0]\n",
    "\n",
    "# Get SHAP values for the sample\n",
    "shap_values_single = explainer(sample)\n",
    "\n",
    "# Plot waterfall\n",
    "shap.plots.waterfall(shap_values_single)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
